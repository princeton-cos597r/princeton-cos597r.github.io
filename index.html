<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>COS 597R: Deep Dive into Large Language Models</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
    <link rel="icon" type="image/png" href="./image/princeton.png">
    <link href="./css/bootstrap.css" rel="stylesheet" media="screen">
    <link href="./css/non-responsive.css" rel="stylesheet">

<style type="text/css">
  body {
    padding-top: 0px;
    padding-bottom: 0px;
  }
  .sidebar-nav {
    padding: 0px 0;
  }
  .page-header {
    border-bottom: 1px solid #ADA9A9;
    margin-bottom: 10px;
  }

  .topic-column {
    column-width: 80em;
  }

  .recommended-column {
    column-width: 100em;
  }
</style>

<style type="text/css">
</style>

</head>

<body>


<div class="container">
    <div class="row">
    <div class="col-lg-15">

      <div class="page-header">
      <h2><b>COS 597R (Fall 2024): Deep Dive into Large Language Models</b></h2> <br>
      </div>

      <div class="bs-component">
      <table class="table table-bordered table-striped">
        <tbody>
        <tr>
        <td>Instructors</td> <td><a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a> (danqic AT cs.princeton.edu) and <a href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a> (arora AT cs.princeton.edu)</td>
        </tr>
        <tr>
        <td>Teaching assistants</td> <td><a href="https://adithyabh.github.io/">Adithya Bhaskar</a> (adithyab AT princeton.edu) and <a href="https://tylerzhu.com/research/">Tyler Zhu</a> (tylerzhu AT princeton.edu)</td>
        </tr>
        <tr>
        <td>Lectures</td> <td>Monday/Wednesday 10:30-11:50am</td>
        </tr>
        <tr>
        <td>Location</td> <td><a href="https://www.cs.princeton.edu/general/images/105-auditorium-0">CS Building 105</a></td>
        </tr>
        <tr>
        <td>Office hours</td> 
        <td>Danqi's office hour: Tuesday 10-11, COS 412 (<a href="https://calendly.com/danqic/office-hours">by appointment</a>) <br> 
            Sanjeev's office hour: Wednesday 4-5pm, COS 407<br> 
            Adithya's office hour: Thursday 3-4pm, Friend 010B <br>
            Tyler's office hour: Monday 4-5pm, Friend 010C 
        </td>
        </tr>
        <tr>
        <td>Feedback form</td><td><a href="https://forms.gle/vUD1RieC1YcBSugw7">https://forms.gle/vUD1RieC1YcBSugw7</a></td>
        </tr>
<!--         <td>Zoom Link (Princeton only)</td><td><a href="https://princeton.zoom.us/j/95623350286">https://princeton.zoom.us/j/95623350286</a></td>
        <tr> -->
        </tbody>
        </table>

        <p><b>We will use a Slack team for most communications this semester. You will be added to the Slack team after the first week. If you join the class late, just email us, and we'll add you. Once you're on Slack, we prefer Slack messages over emails for all logistical questions. We also encourage students to use Slack for discussions related to lecture content and projects.</b><br></p>

        <!-- <p><b>For those of you on the waitlist, unfortunately the classroom is at maximum capacity already, and thus you will not be able to audit in-person. We are currently working with the department to potentially get a larger room. Until then, please use the above zoom link which will live-stream the first few lectures.</b><br></p> -->

        <p>Large language models (LLMs) have revolutionized natural language processing by enabling machines to generate, understand, and interact with human language in more sophisticated ways than ever before. Beyond technical advancements, LLMs are shaping societal interactions with technology, from enhancing accessibility for underserved communities to transforming education, healthcare, and creative industries. This course aims to provide a rigorous survey of current LLM research, including model architecture, data preparation, pre-training, post-training, alignment, and model deployment. The course focuses on conceptual understanding and research rather than engineering, and it is expected to be highly interactive.  Students are expected to read cutting-edge research papers regularly, participate in class discussion, and also complete a major project (in groups of 2-3) at the end, for which computational resources will be arranged. 
        </p>

        <p><b>Prerequisites:</b> COS484 or equivalent background (i.e., familiarity with fundamentals of deep learning/machine learning, Transformers, PyTorch). Open to all graduate students. Undergraduates need instructors' permission.</p>

      <h3><b>Course structure</b></h3>
      <ul>
          <li><b>Class participation (30%)</b>: In each class, we will cover 1-2 papers (see "required reading" in the schedule). You are required to read these papers in depth beforehand, and answer a pre-lecture question form before the class (there is a Google form linked in the schedule). These are due at <b>11:59pm on the day before the lecture</b>. Some questions are designed to test your understanding of the reading materials, and some questions are open-ended and prompt you to read the paper critically and write down your thoughts. This counts towards class participation - we will not grade the correctness but we will expect you to do the work, and submit reasonable answers. 
          </li>
          <li><b>Debate (15%)</b>: We will schedule 12 debate panels in the class from Week 4 to Week 9, with each panel consisting of 5 students and lasting 30 minutes (the lectures will be reduced to 50 minutes). Each panel will focus on one research paper (or two) related to the topics that have been taught so far, and will comprise of the following structure: 
            <ul>
                <li>Each panel will be composed of 1 presenter, 2 critics, and 2 proponents.</li>
                <li>The presenter will start with a short presentation (8 minutes) of the paper.</li>
                <li>The 2 critics will then critique the paper, similar to how reviewers assess conference papers—highlighting limitations, weaknesses, and any claims that are not well supported by the experiments.</li>
                <li>The 2 proponents will explain why they believe the problem does not exist or is not serious.</li>
                <li>There will be multiple rounds of interaction. critics are asked to send their major criticisms to the proponents at least 2 days before the lecture day, so the proponents have time to research and prepare their responses.</li>
                <li><b>The group will write a 2-page summary of the debate later and submit it.</b></li>
            </ul>
          </li>
          <li><b>Lecture scribing (10%)</b>: For each lecture, we will ask 3 students to scribe the lecture content, covering the technical content and research questions.
            <ul>
                <li>You can find the Overleaf scribe template <a href="https://www.overleaf.com/read/yjmgcqgyhsyw#99652f">here</a>. Make a shared copy between all the scribes for a given lecture. It is up to you how to divide up the work so that it is equal. Send your completed Overleaf link + PDF to Adithya and Tyler on Slack <b>by 11:59pm three days after the lecture</b>. For Monday lectures, this is 11:59pm on Thursday. For Wednesday Lectures, this is 11:59pm on Saturday.</li>
                <li>Please do not add the four course instructors on the Overleaf, but instead share the editable link with Adithya and Tyler.</li>
                <li>New to the template is a <b>contributions section</b>, please do fill this out when you submit with an overview of each scribe's split.</li>
            </ul></li>
          <li><b>Final project (35% + 10% for presentations)</b>: At the end of the semester, everyone is required to do a class project related to modern LLMs and submit a final paper. You should work as a <b>team of 2 or 3</b>. Everyone is required to submit a proposal to Gradescope by <b>Oct 13th (Sunday) 11:59pm</b>, and the final paper on the Dean's Date (<b>Dec 13th 11:59 pm</b>). In-class project presentations will be scheduled in the last three lectures.
            The template for the final report is <a href="https://www.overleaf.com/read/smnpdcycbzvg#73a572">here</a>. 
            Feel free to use it for the proposal as well, but you can also use any template you like.
          </li><br>
      </ul>

      <h3><b>Schedule</b></h3>
      <a name="class" id="class"></a>
        <table class="table table-bordered table-striped">
        <thead>
        <tr>
          <th>Date</th>
          <th>Instructor</th>
          <th class="topic-column">Topic/required reading</th>
          <th class="recommended-column">Recommended reading</th>
          <th>Reading response</th>
          <th>Panel discussion</th>
          <th>Scribes</th>
        </tr>
        </thead>
        <tbody>

<!-- week 1 -->
<tr><td>Sep 4 (Wed)</td>
        <td>Sanjeev</td>
        <td>
        Introduction <a href="lectures/lec01.pdf">[slides]</a>
        </td>
        <td></td>
        <td></td>
        <td>N/A</td>
        <td></td>
        </tr>
<!-- week 2 -->
<tr>
    <td>
     Sep 9 (Mon)
    </td>
    <td>
     Danqi
    </td>
    <td>
     Pretraining 1 <a href="lectures/lec02.pdf">[slides]</a>
     <br/>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/2005.14165">
        Language Models are Few-Shot Learners
       </a>
       (GPT-3)
      </li>
     </ul>
    </td>
    <td>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/1706.03762">
        Transformers
       </a>
      </li>
      <li>
        <a href="https://nlp.seas.harvard.edu/annotated-transformer/">
          The Annotated Transformer
        </a>
      </li>
      <li>
       <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">
        GPT-2
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/abs/1810.04805">
        BERT
       </a>
      </li>
      <li>
        <a href="https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising">
          What happened to BERT & T5? (Yi Tay)
        </a>
      </li>
     </ul>
    </td>
    <td>
      <a href="https://forms.gle/UpFZZLQQ6WcJQSbA8">[link]</a>
    </td>
    <td>
     N/A
    </td>
    <td>
      <!-- <a href="lectures/lec02_notes.pdf">[scribe notes]</a> -->
     <ul><li>Yinghui He</li><li>Haichen Dong</li><li>Brendan Y. Wang</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Sep 11 (Wed)
    </td>
    <td>
     Danqi
    </td>
    <td>
     Pretraining 2 <a href="lectures/lec03.pdf">[slides]</a>
     <br/>
     <ul>
      <li><a href="https://arxiv.org/abs/2005.14165">
        Language Models are Few-Shot Learners
       </a> (cont'd)
      </li>
      <li>
       <a href="https://arxiv.org/abs/2407.21783">
        The Llama 3 Herd of Models
       </a>
       , Sections 1-2, Section 3.1-3.2, 3.4, and 5.1
      </li>
     </ul>
    </td>
    <td>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/2310.06825">
        Mistral 7B
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/abs/2402.00838">
        OLMo
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/abs/2407.10671">
        Qwen2
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/abs/2406.03476">
        Data annealing (Databricks)
       </a>
      </li>
     </ul>
    </td>
    <td>
      <a href="https://forms.gle/tWCzZP7E5pRVJ9MQ6">[link]</a>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Jiaxin Xiao</li><li>Dillon Lue</li><li>Ziyu Xiong</li></ul>
    </td>
   </tr>
   <!-- week 3 -->
   <tr>
    <td>
     Sep 16 (Mon)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     Scaling laws <a href="/lectures/lec04.pdf">[slides]</a>
     <br/>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/2203.15556">
        Training Compute-Optimal Large Language Models
       </a> (Chinchilla)
      </li>
      <li>
       <a href="https://arxiv.org/abs/2305.16264">
        Scaling Data-Constrained Language Models
       </a>
      </li>
     </ul>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://arxiv.org/abs/2001.08361">
            Scaling Laws for Neural Language Models
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2401.00448">
            Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws
          </a>
        </li>
      </ul>
    </td>
    <td>
        <a href="https://forms.gle/WCS4JAbAMKP32qju5">
            [link]
        </a>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Wuwei Zhang</li><li>Simran Kaur</li><li>Keerthana Nallamotu</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Sep 18 (Wed)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     Emergent behavior <a href="lectures/lec05.pdf">[slides]</a>
     <br/>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/2206.07682">
        Emergent Abilities of Large Language Models
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/pdf/2307.15936">
        A Theory for Emergence of Complex Skills in Language Models,
       </a>
       Sections 1-3 and 6-8. No need to understand the math.
      </li>
     </ul>
    </td>
    <td>
        <ul>
            <li>
                <a href="https://en.wikipedia.org/wiki/Emergence">
                    Wikipedia entry on Emergence
                </a>
            </li>
             <li>
                <a href="https://arxiv.org/abs/2310.17567">
                  Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models  
                </a>
            </li>
        </ul>
    </td>
    <td>
        <a href="https://forms.gle/UYCC6ro1SfoqgqyE6">
            [link]
        </a>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Erich Liang</li><li>Heyu Guo</li><li>Benedikt P. Stroebl</li></ul>
    </td>
   </tr>
   <!-- week 4 -->
   <tr>
    <td>
     Sep 23 (Mon)
    </td>
    <td>
     Danqi
    </td>
    <td>
     Data curation <a href="lectures/lec06.pdf">[slides]</a>
     <br/>
     <ul>
      <li>
       <a href="https://arxiv.org/abs/2402.00159">
        Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research
       </a>
      </li>
     </ul>
    </td>
    <td>
      <ul>
      <li> <a href="https://arxiv.org/abs/2406.17557">FineWeb</a></li>
      <li> <a href="https://arxiv.org/abs/2306.01116">RefinedWeb</a></li>
      <li> <a href="https://arxiv.org/abs/2406.11794">DataComp-LM</a></li>
      <li> <a href="https://arxiv.org/abs/2402.09739">QuRating</a></li>
    </ul>
    </td>
    <td>
        <a href="https://docs.google.com/forms/d/e/1FAIpQLSelzpIYZxHb06nTXbwmV0DAZb_iq0U5MWYIBziU0OBahWZ6ZQ/viewform">
            [link]
        </a>
    </td>
    <td>
     Paper: <a href="https://arxiv.org/abs/2309.05463">Phi-1.5</a> <b>"More data or better data?"</b><br/>
     Presenter: Victor Chu<br/>
 Critics: <ul><li>Erich Liang</li><li>Tanvi Namjoshi</li></ul>
 Proponents: <ul><li>Simran Kaur</li><li>Tedi Zadouri</li></ul>
    </td>
    <td>
     <ul><li>Sijia Liu</li><li>Iain D. Campbell</li><li>Elizabeth A. Mieczkowski</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Sep 25 (Wed)
    </td>
    <td>
     Danqi
    </td>
    <td>
      Post-training: Instruction tuning <a href="lectures/lec07.pdf">[slides]</a>
      <ul>
      <li> <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a></li>
     </ul>
    </td>
    <td>
      <ul>
      <li><a href="https://arxiv.org/abs/2109.01652">FLAN</a></li>
      <li><a href="https://arxiv.org/abs/2301.13688">The Flan Collection</a></li>
      <li><a href="https://arxiv.org/abs/2306.04751">Tülu</a></li>
      <li><a href="https://arxiv.org/abs/2311.10702">Tulu 2</a></li>
      <li><a href="https://arxiv.org/abs/2402.04333">LESS</a></li>
      <li>Sebastian Ruder's blog posts: <a href="https://newsletter.ruder.io/p/instruction-tuning-vol-1">[1]</a><a href="https://newsletter.ruder.io/p/instruction-tuning-vol-2">[2]</a></li>
     </ul>
    </td>
    <td>
      <a href="https://forms.gle/tLrDiy9AgTVhAN6C7">[link]</a>
    </td>
    <td>
     Paper: <a href="https://arxiv.org/abs/2304.15004">Schaeffer et al 2023</a> <b>"Are emergent abilities a mirage?"</b><br/>
     Presenter: Mingqian Xue<br/>
 Critics: <ul><li>Lekang Yuan</li><li>Heyu Guo</li></ul>
 Proponents: <ul><li>Qishuo Yin</li><li>Lihan Zha</li></ul>
    </td>
    <td>
     <ul><li>Jane E. Castleman</li><li>Kylie Zhang</li><li>Yingqing Guo</li></ul>
    </td>
   </tr>
   <!-- week 5 -->
   <tr>
    <td>
     Sep 30 (Mon)
    </td>
    <td>
     Danqi
    </td>
    <td>
      Post-training: learning from preferences <a href="lectures/lec08.pdf">[slides]</a>
      <ul>
        <li>
       <a href="https://arxiv.org/abs/2203.02155">
       Training language models to follow instructions with human feedback
       </a>
     </li>
       <li>
       <a href="https://arxiv.org/abs/2305.18290">
        Direct Preference Optimization: Your Language Model is Secretly a Reward Model
       </a>
      </li>
    </ul>
    </td>
    <td>
      <ul>
      <li>
       <a href="https://arxiv.org/abs/2406.09279">
        Unpacking DPO and PPO
       </a>
      </li>
      <li>
       <a href="https://arxiv.org/abs/2407.21783">
        Llama 3
       </a>
       , Section 4
      </li>
      <li>
       <a href="https://arxiv.org/abs/2405.14734">
        SimPO
       </a>
      </li>
     </ul>
    </td>
    <td>
      <a href="https://forms.gle/yUgPkjBc6BwpkwN27">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2404.07177">Scaling Laws for Data Filtering</a><br>
     Presenter: Tamjeed Azad<br/>
 Critics: <ul><li>Elizabeth A. Mieczkowski</li><li>Nimra Nadeem</li></ul>
 Proponents: <ul><li>Iain D. Campbell</li><li>Zhicheng Zheng</li></ul>
    </td>
    <td>
     <ul><li>Kincaid MacDonald</li><li>Amey P. Pasarkar</li><li>Nobline Yoo</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Oct 2 (Wed)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     Alignment <a href="lectures/lec9-Alignment.pdf">[slides]</a>
      <ul>
      <li> <a href="https://arxiv.org/abs/2112.00861">A General Language Assistant as a Laboratory for Alignment</a></li>
     </ul>
    </td>
    <td>
        <ul>
            <li>
                <a href="https://dibyaghosh.com/blog/probability/kldivergence.html">The RL probabilist blog on forward and reverse KL</a>
            </li>
        </ul>
    </td>
    <td>
        <a href="https://forms.gle/7v4GaXfJhMgJKadK9">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a><br>
     Presenter: <s>Mahsa Bastankhah</s><br/>
 Critics: <ul><li>Niusha Moshrefi</li><li>Zeyu Shen</li></ul>
 Proponents: <ul><li>Jiaxin Xiao</li><li>Wuwei Zhang</li></ul>
    </td>
    <td>
     <ul><li>Nimra Nadeem</li><li>Stanley Wei</li><li>Cyrus Vachha</li></ul>
    </td>
   </tr>
   <!-- week 6 -->
   <tr>
    <td>
     Oct 7 (Mon)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     Constitutional AI <a href="lectures/lec10-ConstitutionalAI.pdf">[slides]</a>
      <ul>
      <li> <a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></li>
     </ul>
    </td>
    <td>
        <ul>
            <li>
                <a href="https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment">HHH Dataset (just look at some examples)</a>
            </li>
        </ul>
    </td>
    <td>
        <a href="https://forms.gle/LTijRvpGhELEQmbi7">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2404.10719">Is DPO Superior to PPO for LLM Alignment?</a><br>
     Presenter: Boyi Wei<br/>
 Critics: <ul><li>Xingyu Zhu</li><li>Cyrus Vachha</li></ul>
 Proponents: <ul><li>Benedikt P. Stroebl</li><li>Kincaid MacDonald</li></ul>
    </td>
    <td>
     <ul><li>Juhyun Park</li><li>Wentao Guo</li><li><s>Mahsa Bastankhah</s></li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Oct 9 (Wed)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     LLM Metacognition <a href="lectures/lec11-Metacognition.pdf">[slides]</a>
     <ul>
        <li>
            <a href="https://arxiv.org/abs/2405.12205">Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</a>
        </li>
     </ul>
    </td>
    <td>
        <ul>
            <li>
                <a href="https://arxiv.org/abs/2407.21009">AI-Assisted Generation of Difficult Math Questions</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2408.14774">Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning</a>
            </li>
        </ul>
    </td>
    <td>
        <a href="https://forms.gle/muxedaKSMxhPK9qS8">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2406.06560">Inverse Constitutional AI: Compressing Preferences into Principles</a><br>
     Presenter: Zixuan Wang<br/>
 Critics: <ul><li><s>Rafael Pastrana Jimenez</s></li><li>Dillon Lue</li></ul>
 Proponents: <ul><li>Sreemanti Dey</li><li>Jane E. Castleman</li></ul>
    </td>
    <td>
     <ul><li>Wenzhe Li</li><li>Mingqian Xue</li><li><s>Rafael Pastrana Jimenez</s></li></ul>
    </td>
   </tr>
   <!-- week 7 -->
   <tr>
    <td>
     Oct 21 (Mon)
    </td>
    <td>
     Tianyu Gao
    </td>
    <td>
     Long-context models
        <ul>
            <li>
                <a href="https://arxiv.org/abs/2410.02660">How to Train Long-Context Language Models (Effectively)</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>
            </li>
        </ul>
    </td>
    <td>
        <ul>
            <li>
                <a href="https://arxiv.org/abs/2409.12181">A Controlled Study on Long Context Extension and Generalization in LLMs</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2404.06654">RULER: What’s the Real Context Size of Your Long-Context Language Models?</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2309.16039">Effective Long-Context Scaling of Foundation Models</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2402.10171">Data Engineering for Scaling Language Models to 128K Context</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2309.17453">Efficient Streaming Language Models with Attention Sinks</a>
            </li>
        </ul>
    </td>
    <td>
        <a href="https://docs.google.com/forms/d/e/1FAIpQLScW_INl0S7eg5Qa9POwL7884MRDY6Fb5MWidIT_-qDVpkPZKg/viewform?usp=sf_link">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a> <br>
     Presenter: Arin J. Mukherjee<br/>
 Critics: <ul><li>Seth Karten</li><li>Veniamin Veselovskyy</li></ul>
 Proponents: <ul><li>Yuka Shu</li><li>Keerthana Nallamotu</li></ul>
    </td>
    <td>
     <ul><li>Victor Chu</li><li>Yijun Yin</li><li>Lihan Zha</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Oct 23 (Wed)
    </td>
    <td>
     Sanjeev
    </td>
    <td>
     Advanced topics in alignment
        <ul>
            <li>
                <a href="https://cdn.openai.com/o1-system-card-20240917.pdf">OpenAI o1 System Card</a> (skim this and note anything interesting)
            </li>
            <li>
                <a href="https://arxiv.org/abs/2312.09390">Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a> (Read through section 4.2 + skim the rest)
            </li>
        </ul>
    </td>
    <td>
        The AI through debate <a href="https://openai.com/index/debate/">blog post</a> and <a href="https://futureoflife.org/podcast/ai-alignment-through-debate-with-geoffrey-irving/">interview</a>.
    </td>
    <td>
        <a href="https://docs.google.com/forms/d/e/1FAIpQLSdw2GFckWkuOHQ44uxj2Hc2kQb-IyZifmUq_TBzxodlx5cZaw/viewform?usp=sf_link">[link]</a>
    </td>
    <td>
      Paper: <a href="https://arxiv.org/abs/2305.19466">The Impact of Positional Encoding on Length Generalization in Transformers</a> <br>
     Presenter: Ambri Ma<br/>
 Critics: <ul><li>Colin Wang</li><li>Jiahao Qiu</li></ul>
 Proponents: <ul><li>Brendan Y. Wang</li><li>David B. Braun</li></ul>
    </td>
    <td>
     <ul><li>Zeyu Shen</li><li>Tedi Zadouri</li><li>Lekang Yuan</li></ul>
    </td>
   </tr>
   <!-- week 8 -->
   <tr>
    <td>
     Oct 28 (Mon)
    </td>
    <td>
     TBD
    </td>
    <td>
     Topic TBD
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     Presenter: Jiayi Zhang<br/>
 Critics: <ul><li>Catherine Cheng</li><li>Juhyun Park</li></ul>
 Proponents: <ul><li>Wentao Guo</li><li>Sijia Liu</li></ul>
    </td>
    <td>
     <ul><li>Niusha Moshrefi</li><li>Zhicheng Zheng</li><li>Zixuan Wang</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Oct 30 (Wed)
    </td>
    <td>
     TBD
    </td>
    <td>
     Topic TBD
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     Presenter: Constantin Schesch<br/>
 Critics: <ul><li>Yinghui He</li><li>Yijun Yin</li></ul>
 Proponents: <ul><li>Haichen Dong</li><li>Amey P. Pasarkar</li></ul>
    </td>
    <td>
     <ul><li>Creston A. Brooks</li><li>Jiayi Zhang</li><li>Qishuo Yin</li></ul>
    </td>
   </tr>
   <!-- week 9 -->
   <tr>
    <td>
     Nov 4 (Mon)
    </td>
    <td>
     TBD
    </td>
    <td>
     Topid TBD
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     Presenter: Ziyu Xiong<br/>
 Critics: <ul><li>Nobline Yoo</li><li>Creston A. Brooks</li></ul>
 Proponents: <ul><li>Stanley Wei</li><li>Lucy He</li></ul>
    </td>
    <td>
     <ul><li>David B. Braun</li><li>Boyi Wei</li><li>Arin J. Mukherjee</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Nov 6 (Wed)
    </td>
    <td>
     Mengzhou
    </td>
    <td>
     Small models
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     Presenter: Alexandre Kirchmeyer<br/>
 Critics: <ul><li>Wenzhe Li</li><li>Kylie Zhang</li></ul>
 Proponents: <ul><li>Yingqing Guo</li><li>Joie Y . Zhang</li></ul>
    </td>
    <td>
     <ul><li>Sreemanti Dey</li><li>Xingyu Zhu</li><li>Colin Wang</li></ul>
    </td>
   </tr>
   <!-- week 10 -->
   <tr>
    <td>
     Nov 11 (Mon)
    </td>
    <td>
     Guest
    </td>
    <td>
     Guest Lecture #1
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Alexandre Kirchmeyer</li><li>Lucy He</li><li>Jiahao Qiu</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Nov 13 (Wed)
    </td>
    <td>
     Guest
    </td>
    <td>
     Guest Lecture #2
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Veniamin Veselovskyy</li><li>Tanvi Namjoshi</li><li>Ambri Ma</li></ul>
    </td>
   </tr>
   <!-- week 11 -->
   <tr>
    <td>
     Nov 18 (Mon)
    </td>
    <td>
     Guest
    </td>
    <td>
     Guest Lecture #3
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Tamjeed Azad</li><li>Seth Karten</li><li>Catherine Cheng</li></ul>
    </td>
   </tr>
   <tr>
    <td>
     Nov 20 (Wed)
    </td>
    <td>
     Guest
    </td>
    <td>
     Guest Lecture #4
    </td>
    <td>
    </td>
    <td>
    </td>
    <td>
     N/A
    </td>
    <td>
     <ul><li>Constantin Schesch</li><li>Yuka Shu</li><li>Joie Y . Zhang</li></ul>
    </td>
   </tr>

<!-- week 12 -->
<tr><td>Nov 25 (Mon)</td>
  <td>Students</td>
  <td>
  Project presentations
  </td>
  <td>
  </td>
  <td></td>
  <td>N/A</td>
  <td></td>
</tr>

<!-- week 13 -->
<tr><td>Dec 2 (Mon)</td>
  <td>Students</td>
  <td>
  Project presentations
  </td>
  <td>
  </td>
  <td></td>
  <td>N/A</td>
  <td></td>
</tr>

<tr><td>Dec 4 (Wed)</td>
  <td>Students</td>
  <td>
  Project presentations
  </td>
  <td>
  </td>
  <td></td>
  <td>N/A</td>
  <td></td>
</tr>

</div><!--container-->
<script src="./css/jquery-1.10.2.min.js"></script>
<script src="./css/bootstrap.min.js"></script>
</body></html>
